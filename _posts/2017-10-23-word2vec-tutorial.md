---
layout: post
title:  "Word2vec Tutorial"
date:   2017-10-23 10:07:24 +0800
categories: jekyll update
mathjax: true
---

word2vec æ˜¯ç”± Tomas Mikolov ç­‰äººç™¼å±•å‡ºä¾†ï¼ŒæŠŠè©è½‰æ›æˆå‘é‡çš„ä¸€ç¨®æ¼”ç®—æ³•ï¼Œä½¿ç”¨ä¸‰å±¤æ¶æ§‹çš„ç¥ç¶“ç¶²è·¯ï¼Œè¼¸å…¥å±¤èˆ‡è¼¸å‡ºå±¤å¤§å°éƒ½æ˜¯è¨“ç·´æ–‡æœ¬çš„è©æ•¸ï¼Œä¸­é–“å±¤å¤§å°å°±æ˜¯æƒ³è¦å£“ç¸®çš„å‘é‡å¤§å°ï¼Œå¹³å¸¸ç¥ç¶“ç¶²è·¯è¨“ç·´å®Œæˆ‘å€‘éƒ½æ˜¯æƒ³è¦æœ€çµ‚çš„è¼¸å‡ºçµæœï¼Œä½† word2vec
è¨“ç·´å®Œçš„æ¬Šé‡æ‰æ˜¯æˆ‘å€‘æƒ³è¦çš„æ±è¥¿ï¼Œç‚ºä»€éº¼æ˜¯é€™æ¨£å‘¢ï¼Ÿæˆ‘æ¥ä¸‹ä¾†ç›´æ¥ç”¨ä¾‹å­ä¾†è§£é‡‹ï¼Œé€™ç¯‡æ•™å­¸æœƒè¬›åˆ°æ•¸å­¸æ¨å°å’Œå¯¦ä½œï¼Œç¨‹å¼ç¢¼åœ¨[é€™](https://github.com/ctjoy/word2vec_tutorial)ã€‚

# Introduction

å‡å¦‚ç¾åœ¨æ–‡æœ¬åªæœ‰ 2 è¡Œ 5 ç¨®è©ï¼Œå¦‚ä¸‹

```
ç‹—æœƒè·‘
é­šæœƒæ¸¸
```

æ¯å€‹è©éƒ½æƒ³è¦å£“ç¸®æˆ 3 ç¶­çš„å‘é‡ï¼Œé‚£æ¨¡å‹çš„æ¶æ§‹å°±æœƒé•·é€™æ¨£

<p align="center">
<img src="/assets/images/2017-10-23-word2vec-tutorial/structure.png" width="900">
</p>

æ¬Šé‡ v å’Œ w åˆå§‹å€¼éš¨æ©Ÿçµ¦ï¼Œæˆ‘å€‘æ‹¿åˆ°æ–‡æœ¬å¾Œå°±å¯ä»¥æŠŠæ¯å€‹è©ç”¨ one-hot encoding çš„æ–¹å¼è½‰æ›

$$ ç‹— = \begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ \end{bmatrix} 
æœƒ = \begin{bmatrix} 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ \end{bmatrix}
è·‘ = \begin{bmatrix} 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ \end{bmatrix} 
é­š = \begin{bmatrix} 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ \end{bmatrix}
æ¸¸ = \begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ \end{bmatrix} $$

é€™æ™‚å¦‚æœæˆ‘å€‘è¦å°ã€Œç‹—ã€é€™å€‹è©é€²è¡Œè¨“ç·´ï¼Œå°±æœƒå¯ä»¥çŸ¥é“ä»¥ v é€™å€‹æ¬Šé‡ä¾†èªªï¼Œæœƒéœ€è¦æ›´æ–°çš„åªæœ‰ \\( \begin{bmatrix} v_{11} & v_{12} & v_{13} \end{bmatrix} \\)

$$ \begin{bmatrix} 1 & 0  & 0 & 0 & 0 \end{bmatrix} \begin{bmatrix} v_{11} & v_{12} & v_{13} \\ 
                                                                    v_{21} & v_{22} & v_{23} \\ 
                                                                    v_{31} & v_{32} & v_{33} \\ 
                                                                    v_{41} & v_{42} & v_{43} \\ 
                                                                    v_{51} & v_{52} & v_{53} \end{bmatrix}
                                                                    = \begin{bmatrix} v_{11} & v_{12} & v_{13} \end{bmatrix}$$

è‡³æ–¼ w é‚£é‚Šä¹Ÿæ˜¯ä¸€æ¨£ï¼Œæ‰€ä»¥å¯¦éš›ä¸Šæœ€å¾Œè¨“ç·´å‡ºä¾†çš„æ¬Šé‡ v å’Œ w éƒ½å¯ä»¥ç”¨ï¼Œåªæ˜¯å¤§éƒ¨åˆ†ç¿’æ…£éƒ½åªæ‹¿ v ç•¶æœ€å¾Œçš„å£“ç¸®å‘é‡ã€‚

# Intuition

åœ¨ç¥ç¶“ç¶²è·¯ä¸­æœ€é‡è¦çš„å°±æ˜¯ loss function äº†ï¼Œword2vec çš„ loss function å®šç¾©ç‚º

$$ J = -\log \sigma(v_{I}^{T} w_{pos}) - \sum_{neg} \mathbf{E}_{w_{neg} \sim P_{n}(I)} \left[ \log \sigma(-v_{I}^{T} w_{neg}) \right ] $$

\\(\sigma \\) æ˜¯ sigmoid functionï¼Œ\\(v_{I}\\) ä»£è¡¨ input word çš„æ¬Šé‡å‘é‡ï¼Œ\\(w_{pos}\\) ä»£è¡¨ positive sample çš„æ¬Šé‡å‘é‡ï¼Œ\\(w_{neg}\\) ä»£è¡¨ negative sample çš„æ¬Šé‡å‘é‡ï¼Œ\\(P_{n}\\) ä»£è¡¨ä¸€å€‹åˆ†ä½ˆï¼Œnegative sample å¾é€™å€‹åˆ†ä½ˆè¢«å‡ºæŠ½å‡ºä¾†ã€‚

é‚£ä»€éº¼æ˜¯ positive sample å’Œ negative sampleï¼Ÿ word2vec æ˜¯ä¸€å€‹è©è¢‹æ¨¡å‹ï¼Œå¯ä»¥æƒ³åƒè·Ÿç›®æ¨™è©åœ¨åŒä¸€å€‹è¢‹å­è£¡çš„å°±æ˜¯ positive sampleï¼Œä¸åœ¨è¢‹å­è£¡çš„å°±æ˜¯ negative sampleï¼Œæ˜¯å¦åœ¨è¢‹å­è£¡ç”¨ window size å»å®šç¾©ï¼Œå‡å¦‚ window size æ˜¯ 1ï¼Œå°±ä»£è¡¨ç›®æ¨™è©ä¸Šä¸‹ä¸€å€‹è©æ˜¯åœ¨åŒä¸€å€‹è¢‹å­è£¡ï¼Œä¾‹å¦‚ã€Œç‹—æœƒè·‘ã€é€™å¥ï¼Œå°ã€Œç‹—ã€ä¾†èªªåŒä¸€å€‹è¢‹å­è£¡çš„å°±æœ‰ã€Œæœƒã€é€™å€‹è©ã€‚

é‚£ç‚ºä»€éº¼ loss function è¦é€™æ¨£å®šç¾©å‘¢ï¼Ÿå°±è¦ä¾†çœ‹çœ‹ä»€éº¼æƒ…æ³ä¸‹ loss function æœƒè¶¨è¿‘æ–¼ 0ï¼Œæˆ‘å€‘éƒ½çŸ¥é“ log0 ç­‰æ–¼è² ç„¡é™å¤§ï¼Œlog1 ç­‰æ–¼ 0ï¼Œsigmoid function åˆæ˜¯å€‹ä»‹æ–¼ 0 åˆ° 1 ä¹‹é–“çš„å‡½æ•¸ï¼Œæ‰€ä»¥ä»£è¡¨ \\(J\\) è¦è¶¨è¿‘æ–¼ 0 å°±ä»£è¡¨ï¼Œ\\(\sigma(v_{I}^{T} w_{pos})\\) å’Œ \\(\sigma(-v_{I}^{T} w_{neg}) \\) éƒ½è¦è¶¨è¿‘æ–¼ 1ï¼Œé€™æ¨£å°±ä»£è¡¨ \\(v_{I}^{T} w_{pos}\\) è¦è¶¨è¿‘æ–¼ç„¡é™å¤§ï¼Œ\\(v_{I}^{T} w_{neg}\\) è¦è¶¨è¿‘æ–¼è² ç„¡é™å¤§ï¼Œæ‰€ä»¥ input word çš„æ¬Šé‡å‘é‡è¦è·Ÿ positive sample çš„æ¬Šé‡å‘é‡å…§ç©è¶Šå¤§è¶Šå¥½ï¼Œä¹Ÿå°±æ˜¯è§’åº¦è¦è¶Šå°è¶Šå¥½ï¼Œè·Ÿ negative sample
çš„æ¬Šé‡å‘é‡å…§ç©è¦è¶Šå°è¶Šå¥½ï¼Œè§’åº¦è¦è¶Šå¤§è¶Šå¥½ã€‚

<p align="center">
<img src="/assets/images/2017-10-23-word2vec-tutorial/vector.gif" width="900">
</p>

ä¸€é–‹å§‹ 5 å€‹è©çš„æ¬Šé‡å‘é‡æ˜¯éš¨æ©Ÿçµ¦çš„ï¼Œç¬¬ä¸€è¼ªè¦è¨“ç·´çš„ input word æ˜¯ã€Œç‹—ã€ï¼Œpositive sampleã€Œæœƒã€ï¼Œnegative sampleã€Œæ¸¸ã€ï¼Œé€™æ™‚å› ç‚ºè¦è®“ loss è®Šå°æ‰€ä»¥ï¼Œç‹—å’Œæœƒçš„æ¬Šé‡å‘é‡è§’åº¦æ‹‰è¿‘ï¼Œç‹—å’Œæ¸¸çš„æ¬Šé‡å‘é‡è§’åº¦åˆ†é–‹ï¼Œç¬¬äºŒè¼ª input word æ˜¯ã€Œé­šã€ï¼Œpositive sampleã€Œæœƒã€ï¼Œnegative sampleã€Œè·‘ã€ï¼Œé­šå’Œæœƒè§’åº¦ç¸®å°ï¼Œé­šå’Œè·‘è§’åº¦è®Šå¤§ï¼Œç¶“éè¨“ç·´æœƒè®“è©ç¾©ç›¸è¿‘çš„å‘é‡æ¥è¿‘ã€‚

# Backpropagation  

äº†è§£ loss function å°±å¯ä»¥ä¾†ç®— gradient æ›´æ–°æ¬Šé‡äº†ï¼Œç‚ºäº†æ–¹ä¾¿å¾Œé¢æ¨å°ï¼Œå…ˆå®šç¾©ä¸€ä¸‹

$$ v = \begin{bmatrix} v_{11} & v_{12} & v_{13} \\ 
                       v_{21} & v_{22} & v_{23} \\ 
                       v_{31} & v_{32} & v_{33} \\ 
                       v_{41} & v_{42} & v_{43} \\ 
                       v_{51} & v_{52} & v_{53} \end{bmatrix}
                      = \begin{bmatrix} v_{1} \\ v_{2} \\ v_{3} \\ v_{4} \\ v_{5}\end{bmatrix}$$

$$ w^T = \begin{bmatrix} w_{11} & w_{12} & w_{13} \\ 
                       w_{21} & w_{22} & w_{23} \\ 
                       w_{31} & w_{32} & w_{33} \\ 
                       w_{41} & w_{42} & w_{43} \\ 
                       w_{51} & w_{52} & w_{53} \end{bmatrix}
                      = \begin{bmatrix} w_{1} \\ w_{2} \\ w_{3} \\ w_{4} \\ w_{5}\end{bmatrix}$$


å»¶çºŒå‰›æ‰çš„ä¾‹å­ï¼Œç¬¬ä¸€è¼ª \\(v_{I}\\)ã€\\(w_{pos}\\)ã€\\(w_{neg}\\) åˆ†åˆ¥æ˜¯ã€Œç‹—ã€ã€ã€Œæœƒã€ã€ã€Œæ¸¸ã€çš„æ¬Šé‡å‘é‡ï¼Œæ‰€ä»¥å°æ–¼ positive çš„éƒ¨åˆ†è¦è¦æ›´æ–°çš„æ¬Šé‡å¦‚ä¸‹

$$ v_{1} \leftarrow v_{1} - \eta \nabla_{v_{1}}J $$

$$ w_{2} \leftarrow w_{2} - \eta \nabla_{w_{2}}J $$

å°æ–¼ negative çš„éƒ¨åˆ†è¦æ›´æ–°çš„æ˜¯

$$ v_{1} \leftarrow v_{1} - \eta \nabla_{v_{1}}J $$

$$ w_{5} \leftarrow w_{5} - \eta \nabla_{w_{5}}J $$

#### Positive

å…ˆçœ‹ \\(v_{1} \leftarrow v_{1} - \eta \nabla_{v_{1}}J\\) é€™éƒ¨åˆ†ï¼Œ\\(J\\) å° \\(v_{1}\\) çš„ gradient å¯ä»¥å¯«æˆ

$$ \nabla_{v_{1}}J = \begin{bmatrix}
\frac{\partial J}{\partial v_{11}}\\
  \frac{\partial J}{\partial v_{12}}\\
  \frac{\partial J}{\partial v_{13}}\\
  \end{bmatrix}$$

å®šç¾©

$$ x_{12} = v_{1}^{T} w_{2}$$

$$ J = -\log \sigma(x_{12}) $$

å°±å¯ä»¥æŠŠå¼å­æ”¹å¯«ä¸€ä¸‹

$$ \frac{\partial J}{\partial v_{11}} = \frac{dJ}{dx_{12}} \frac{dx_{12}}{dv_{11}} $$

$$ \frac{dJ}{dx_{12}} = \frac{d -\log \sigma(x_{12})}{dx_{12}} = - \frac{1}{\sigma(x_{12})} \sigma(x_{12}) (1 - \sigma(x_{12})) = \sigma(x_{12}) - 1$$

$$ \frac{dx_{12}}{dv_{11}} = \frac{d(v_{1}^{T} w_{2})}{dv_{11}} = \frac{d(v_{11} w_{21} + v_{12} w_{22} + v_{13} w_{23})}{dv_{11}} = w_{21}$$

é€™æ™‚å°±å¯ä»¥ä»£å›å»åŸæœ¬çš„

$$ \frac{\partial J}{\partial v_{11}} = (\sigma(x_{12}) - 1)w_{21} = (\sigma(v_{1}^{T} w_{2}) - 1)w_{21}$$

$$ \nabla_{v_{1}}J = \begin{bmatrix}
\frac{\partial J}{\partial v_{11}}\\
  \frac{\partial J}{\partial v_{12}}\\
  \frac{\partial J}{\partial v_{13}}\\
  \end{bmatrix}
 = \begin{bmatrix}
  (\sigma(v_{1}^{T} w_{2}) - 1)w_{21}\\
  (\sigma(v_{1}^{T} w_{2}) - 1)w_{22}\\
  (\sigma(v_{1}^{T} w_{2}) - 1)w_{23}\\
  \end{bmatrix}
  = (\sigma(v_{1}^{T} w_{2}) - 1)w_{2}\$$

æœ€çµ‚å¾—åˆ°

$$ v_{1} \leftarrow v_{1} - \eta (\sigma(v_{1}^{T} w_{2}) - 1)w_{2}$$

è‡³æ–¼ w çš„éƒ¨åˆ†ä¹Ÿæ˜¯ä¸€æ¨£æ­¥é©Ÿï¼Œæ‰€ä»¥å¯ä»¥å¾—åˆ°

$$ w_{2} \leftarrow w_{2} - \eta (\sigma(v_{1}^{T} w_{2}) - 1)v_{1}$$

#### Negative

negative è·Ÿ positive å·®åˆ¥åœ¨æ–¼å¹¾å€‹å°åœ°æ–¹

$$ x_{15} = v_{1}^{T} w_{5}$$

$$ J = -\log \sigma(-x_{15}) $$

æ‰€ä»¥

$$ \frac{dJ}{dx_{15}} = \frac{d -\log \sigma(-x_{15})}{dx_{15}} = - \frac{1}{\sigma(-x_{15})} \sigma(-x_{15}) (1 - \sigma(-x_{15})) = 1 - \sigma(-x_{15}) = \sigma(x_{15})$$

æœ€çµ‚å¾—åˆ°

$$ v_{1} \leftarrow v_{1} - \eta (\sigma(v_{1}^{T} w_{2}) - 0)w_{5}$$

$$ w_{5} \leftarrow w_{5} - \eta (\sigma(v_{1}^{T} w_{5}) - 0)v_{1}$$

#### Update weight

å›æƒ³ä¸€ä¸‹å‰›æ‰åœ¨ intuition çš„éƒ¨åˆ†æœ‰è¬›åˆ° \\(\sigma(v_{I}^{T} w_{pos})\\) å’Œ \\(\sigma(-v_{I}^{T} w_{neg}) \\) éƒ½è¦è¶¨è¿‘æ–¼ 1ï¼Œä¹Ÿå°±æ˜¯ \\(\sigma(v_{I}^{T} w_{pos})\\) è¦è¶¨è¿‘æ–¼ 1ï¼Œ \\(\sigma(v_{I}^{T} w_{neg}) \\) è¦è¶¨è¿‘æ–¼ 0ï¼Œå°ç…§å‰›æ‰ update çš„å…¬å¼ï¼Œæœ‰ç¨®è·Ÿç›®æ¨™å·®ç•°å¤šå°‘å°±çŸ¯æ­£å›ä¾†çš„æ„Ÿè¦ºã€‚

# Implementation

æœ‰äº†å‰é¢çš„çŸ¥è­˜å¾Œæˆ‘å€‘å°±å¯ä»¥å¯¦éš›å¯«ç¨‹å¼ï¼Œè·‘è·‘çœ‹çµæœï¼Œå¾ˆå¤šæ•™å­¸æ–‡ç« éƒ½ç›´æ¥ç”¨ [genism](https://radimrehurek.com/gensim/) é€™å€‹å·²ç¶“å¯«å¥½çš„æ¨¡å‹ä¾†åšå¯¦é©—ï¼Œä½†èº«ç‚ºä¸€å€‹è³‡å·¥ç³»å­¸ç”Ÿé‚„æ˜¯æƒ³è‡ªå·±å‹•æ‰‹åˆ»çœ‹çœ‹ï¼Œæ‰€ä»¥æˆ‘æœ€å¾Œå¯¦ä½œäº†å…©å€‹ç‰ˆæœ¬ï¼Œä¸€æ˜¯è‡ªå·±åˆ»çš„ï¼ˆå¾ˆå¤§éƒ¨åˆ†åƒè€ƒ [Mark çš„ code](http://cpmarkchang.logdown.com/posts/773558-neural-network-word2vec-part-3-implementation)ï¼‰ï¼Œç¬¬äºŒå€‹ç‰ˆæœ¬æ˜¯ç”¨ tensorflow ï¼Œç”±æ–¼ tensorflow å®˜æ–¹å°±æœ‰è‡ªå·±çµ¦ word2vecï¼Œæ‰€ä»¥æˆ‘æ”¹å‹•çš„éƒ¨åˆ†å°±æ˜¯è®€è³‡æ–™çš„éƒ¨åˆ†ï¼Œå…©å€‹ä¸€èµ·å°ç…§æ¯”è¼ƒæœƒæ›´äº†è§£ï¼Œcode å¯ä»¥ç›´æ¥åˆ°[æˆ‘çš„ github](https://github.com/ctjoy/word2vec_tutorial) çœ‹ï¼Œé€™é‚Šè¬›è§£ä¸€äº›å¯¦ä½œæœƒé‡åˆ°çš„ç´°ç¯€ã€‚ 

#### Data  
ä¸­æ–‡è¨“ç·´è©å‘é‡æœƒé‡åˆ°æ–·è©çš„éº»ç…©ï¼Œé›–ç„¶ç¾åœ¨éƒ½å¯ä»¥ç”¨ [jiaba](https://github.com/fxsjy/jieba) é€™å€‹æ–·è©å·¥å…·ä¾†è§£æ±ºï¼Œä½†ç‚ºäº†æ¸›å°‘éº»ç…©æˆ‘å°±é¸ç”¨å”è©©ä¾†ç•¶èªæ–™åº«ï¼Œæˆ‘ç”¨åˆ¥äººå·²ç¶“æ•´ç†å¥½çš„è³‡æ–™ [chinese-poetry](https://github.com/chinese-poetry/chinese-poetry) ä¸­å”è©©çš„éƒ¨åˆ†ã€‚

å‡å¦‚ä¸€å€‹å¥å­ `ç§¦å·é›„å¸å®…`è½‰æ›æˆ index `[149, 65, 199, 250, 345]`ï¼Œwindow_size é¸æ“‡ 1ï¼Œç¶“éå‰è™•ç†å¾Œå°±æœƒè®Šæˆé€™æ¨£ï¼Œå·¦æ˜¯ input wordï¼Œå³æ˜¯ positive sampleã€‚

```python
[149, 65]  #ç§¦ -> å·
[65, 149]  #å· -> ç§¦
[65, 199]  #å· -> é›„
[199, 65]  #é›„ -> å·
[199, 250] #é›„ -> å¸
[250, 199] #å¸ -> é›„
[250, 345] #å¸ -> å®…
[345, 250] #å®… -> å¸
```

#### Unigram distribution  
åœ¨é¸æ“‡è¦ç”¨å“ªäº›è©ç•¶è² æ¨£æœ¬æ™‚è¦æ€éº¼é¸ï¼Ÿä¹Ÿå°±æ˜¯å…¬å¼ä¸­çš„ \\(P_{n}\\) æ˜¯ä»€éº¼ï¼ŸåŸºæœ¬ä¸Šè¨­è¨ˆé€™å€‹åˆ†ä½ˆæ™‚å¸Œæœ›è©è¢«æŠ½åˆ°çš„æ©Ÿç‡è¦è·Ÿé€™å€‹è©å‡ºç¾çš„é »ç‡æœ‰é—œï¼Œå‡ºç¾åœ¨æ–‡æœ¬ä¸­çš„é »ç‡è¶Šé«˜è¶Šé«˜è¶Šæœ‰å¯èƒ½è¢«æŠ½åˆ°ï¼ŒMikolov ä»–å€‘åœ¨ paper ä¸­çµ¦å‡ºä¸€å€‹å…¬å¼

$$ P(w_i) = \frac{ {f(w_i)}^{3/4}  }{\sum_{j=0}^{n}\left( {f(w_j)}^{3/4} \right ) } $$

\\(f(w_i)\\) ä»£è¡¨ \\(w_i\\) é€™å€‹è©å‡ºç¾çš„æ¬¡æ•¸ï¼Œé‚£ç‚ºä»€éº¼æœƒæœ‰ 3/4 æ¬¡æ–¹é€™å€‹æ±è¥¿ï¼Ÿ paper ä¸Šé¢å¯«èªªæ˜¯ä»–å€‘å¯¦é©—å‡ºä¾†è¦ºå¾—æœ€å¥½çš„çµæœï¼Œæˆ‘å€‘ç¨±é€™å€‹åˆ†ä½ˆç‚º unigram distributionï¼Œæ‰€ä»¥å¯ä»¥è£½é€ ä¸€å€‹ unigram tableï¼Œä¾ç…§æŸå€‹è©å‡ºç¾çš„æ¬¡æ•¸ä¹˜ä»¥ 0.75 æ¬¡æ–¹ï¼Œé€™å°±æ˜¯ä»–åœ¨ unigram table å‡ºç¾çš„æ¬¡æ•¸ï¼Œæœ€å¾Œå°±åªè¦å¾é€™å€‹ unigram table éš¨æ©ŸæŠ½æ¨£å°±å¥½äº†ï¼Œä¾‹å¦‚ï¼šæœ‰ä¸€å€‹è©ç·¨è™Ÿæ˜¯ 100ï¼Œå®ƒå‡ºç¾åœ¨æ•´å€‹æ–‡æœ¬ä¸­ 1000 æ¬¡ï¼Œæ‰€ä»¥ 100 åœ¨ unigram table å°±æœƒå‡ºç¾ 1000 ^ 0.75 = 177 æ¬¡ï¼Œå¯¦ä½œå¦‚ä¸‹

```python
def build_unigram_table(word_dict):
  frq = map(lambda v: int(v**0.75), word_dict.values())
  t = [np.full(v, i, dtype=int) for i, v in enumerate(frq)] 

  return np.concatenate(t) # for negative sampling
```

è‡³æ–¼è¦é¸å¹¾å€‹è©ç•¶ negative sampleï¼Œpaper ä¸­å»ºè­°å¦‚ä¸‹
> Our experiments indicate that values of k in the range 5â€“20 are useful for small training datasets, while for large datasets the k can be as small as 2â€“5.

#### Subsampling of frequent words  
è‹±æ–‡ä¸­ "the", "a", "in"ï¼Œä¸­æ–‡ä¸­çš„ã€Œçš„ã€ã€ã€Œæ˜¯ã€ç­‰ç­‰é€™ç¨®è©ï¼Œå…¶å¯¦åœ¨å¥å­ä¸­ä¸¦æ²’æœ‰è¾¦æ³•æä¾›å¤ªå¤šè³‡è¨Šä½†åˆå¸¸å¸¸å‡ºç¾ï¼Œå°è¨“ç·´æ²’æœ‰å¤ªå¤§å¹«åŠ©ï¼Œæ‰€ä»¥å°±ç”¨ä¸€å€‹æ©Ÿç‡ä¾†æ±ºå®šé€™å€‹è©æ˜¯å¦è¦è¢«ä¸Ÿæ‰ï¼Œå…¬å¼å¦‚ä¸‹

$$ P(w_i) = (\sqrt{\frac{z(w_i)}{0.001}} + 1) \cdot \frac{0.001}{z(w_i)} $$

\\(z(w_i)\\) ä»£è¡¨ \\(w_i\\) é€™å€‹è©å‡ºç¾çš„é »ç‡ï¼Œé€™è£¡çš„å…¬å¼è·Ÿ paper çš„ä¸¦ä¸ä¸€æ¨£ï¼Œå› ç‚ºæˆ‘åƒè€ƒ [Chris McCormick çš„ blog ](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)å¾—çŸ¥ Google å¯¦ä½œ word2vec çš„ C code è£¡é¢æ˜¯é€™å€‹å…¬å¼ï¼Œå»ºè­°å¯ä»¥çœ‹çœ‹ Chris McCormick çš„æ–‡ç« ï¼Œä»–æœ‰æ¢è¨ç‚ºä»€éº¼é€™æ¨£è¨­è¨ˆé€™å€‹å…¬å¼ï¼Œæˆ‘è‡ªå·±ç†è§£è¦ºå¾—é€™è£¡åœ¨åšçš„äº‹è·Ÿå»æ‰ stop word æ˜¯ä¸€æ¨£çš„æ¦‚å¿µã€‚

```python
z = word_dict[s[w]] / total_words
keeping_rate = (np.sqrt(z / 0.001) + 1) * (0.001 / z)
if keeping_rate < np.random.uniform(): continue # discard the word appears too frequently
```

#### Using tensorflow  
ç”¨ tensorflow å¯¦ä½œæ™‚ unigram distribution å’Œ backpropagation éƒ½ä¸ç”¨è™•ç†ï¼Œåƒç…§ [Google çµ¦çš„ basic example](https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/examples/tutorials/word2vec/word2vec_basic.py)ï¼Œç›´æ¥ä½¿ç”¨ä¸€å€‹ loss function ç¨±ç‚º [tf.nn.nce_loss](https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss)

```python
loss = tf.reduce_mean(
    tf.nn.nce_loss(weights=nce_weights,
      biases=nce_biases,
      labels=train_labels,
      inputs=embed,
      num_sampled=num_sampled,
      num_classes=vocabulary_size)
```
è§£é‡‹æ¯å€‹åƒæ•¸ï¼š
* weights: ç¶­åº¦ç‚º `(vocabulary_size, hidden_layer_size)`ã€åˆå§‹åŒ–æ˜¯éš¨æ©Ÿå€¼çš„çŸ©é™£
* biases: ç¶­åº¦ç‚º `(vocabulary_size, 1)`ã€åˆå§‹åŒ–æ˜¯ 0 çš„çŸ©é™£ï¼Œå› ç‚º word2vec æ²’æœ‰ç”¨åˆ° biases
* labels: å‰è™•ç†å¾Œçš„è³‡æ–™ï¼Œclass -> label
* inputs: é€™æ¬¡ batch é¸ä¸­è¦è¨“ç·´çš„è©çš„ weight
* num_sampled: è¦é¸å¹¾å€‹è² æ¨£æœ¬
* num_classes: æ–‡æœ¬ä¸­æœ‰å¹¾å€‹è¦è¢«è¨“ç·´çš„è©
* num_true (default=1): ä¸€æ¬¡è¨“ç·´æ™‚ç›®æ¨™çš„é¡åˆ¥æœ‰å¹¾å€‹
* remove_accidental_hits (default=True): åˆæ­¥ç†è§£æ˜¯å¦‚æœåœ¨è² æ¡æ¨£æ™‚é¸åˆ°æ­£æ¨£æœ¬è¦ä¸è¦ä¸Ÿæ‰ï¼Œä½†æ ¹æ“š [Google çµ¦çš„æ–‡ä»¶](https://www.tensorflow.org/extras/candidate_sampling.pdf) æ„Ÿè¦ºé‚„æœ‰ä¸€äº›åˆ¥çš„æ„ç¾©ï¼Œç›®å‰é‚„æ²’ä»”ç´°ç ”ç©¶
* partition_strategy (default='mod'): å° weights æŸ¥è¡¨çš„ç­–ç•¥
* name (default='nce_loss'): çµ¦é€™å€‹å‹•ä½œçš„åå­—ï¼Œå° TensorBoard æœ‰ç”¨
* sampled_values (default=None): è¦çµ¦ä»–è² æ¡æ¨£çš„æ¨£æœ¬ï¼Œå¦‚æœæ˜¯ None ï¼Œä»–æœƒè‡ªå·±é¸æ“‡  [tf.nn.log_uniform_candidate_sampler](https://www.tensorflow.org/api_docs/python/tf/nn/log_uniform_candidate_sampler) é€™å€‹ samplerï¼Œä»–é¸æ“‡è² æ¨£æœ¬çš„æ–¹å¼æ˜¯

$$ P(class) = \frac{log(class + 2) - log(class + 1)}{log(n + 1)} $$

n æ˜¯ vocabulary_sizeï¼Œå‡è¨­æœ‰å€‹æ–‡æœ¬çš„ vocabulary_size æ˜¯ 10000ï¼Œå¯ä»¥åœ¨ Google ç›´æ¥æ‰“ plot (log(x + 2) - log(x + 1)) / log(10000 + 1)ï¼Œè§€å¯ŸçŸ¥é“å¦‚æœä»–çš„ class æ¯”è¼ƒå°å°±æœƒæ¯”è¼ƒå®¹æ˜“è¢«é¸åˆ°ï¼Œæ‰€ä»¥åœ¨æŠŠè©è½‰æ›æˆ index æ™‚ï¼Œè¦ä¾ç…§å‡ºç¾æ¬¡æ•¸æ’åˆ—ï¼Œè·Ÿ unigram distribution çš„æ–¹æ³•ä¸ä¸€æ¨£ï¼Œä½†éƒ½æ˜¯è¶Šå¸¸å‡ºç¾è¶Šå®¹æ˜“è¢«é¸åˆ°çš„æƒ³æ³•ã€‚

<p align="center">
<img src="/assets/images/2017-10-23-word2vec-tutorial/plot_sampler.png" width="500">
</p>

# Result

```
# from scratch               # tensorflow      
# spend: 53.94 min           # spend: 38.09 min 
                             
é›² 1.0                       é›² 1.0
åµ 0.818097894953            åµ 0.731922132211
ç·² 0.807170161919            éœ 0.710187307407
çƒ½ 0.806751349354            çƒŸ 0.693668808384
çƒŸ 0.791932317029            é›ª 0.684637639979
é„ 0.790464066718            è™¹ 0.683235227787
-----                        -----
å³° 1.0                       å³° 1.0
å³¯ 0.96521154438             å³¯ 0.942029995583
å±¤ 0.869375215503            å¶½ 0.73387296403
å·’ 0.847521841138            åµ‹ 0.732944525
å·– 0.842055300736            å·’ 0.716149847575
å·” 0.834164942036            å·” 0.714281751101
-----                        -----
é¢¨ 1.0                       é¢¨ 1.0
é£† 0.839413385589            å¹ 0.820511746298
æ¶¼ 0.812897226871            é£† 0.809179019451
å‡œ 0.790959089145            é€† 0.67986909613
é¢¸ 0.786966264664            é¢¸ 0.663089281948
æš„ 0.771490669881            æ¶¼ 0.659044072466
-----                        -----
å¥³ + çˆ¶ - ç”·                 å¥³ + çˆ¶ - ç”·
æ¯ 0.765840473955            æ¯ 0.735594336365
å©¦ 0.758031202523            å­ 0.729155945201
å­ 0.724152991944            ä¼´ 0.696736003898
ä¼´ 0.707958812532            å½¿ 0.645417693955
é˜¿ 0.702062120972            é˜¿ 0.629788529922
```

# Acknowledgments and references
* [Mark Chang çš„éƒ¨è½æ ¼](http://cpmarkchang.logdown.com/posts/773062-neural-network-word2vec-part-1-overview)å¹«åŠ©æˆ‘å¾ˆå¤šï¼Œå°¤å…¶æ˜¯æˆ‘å¤§é‡åƒè€ƒæ¨å°å…¬å¼èˆ‡ç¬¬ä¸‰ç¯‡å¯¦ä½œ code çš„å…§å®¹ï¼Œéå¸¸æ¨è–¦å€¼å¾—ä¸€è®€
* Chris McCormick çš„éƒ¨è½æ ¼ï¼Œè¬›å¾—éå¸¸ç°¡æ½”æ˜ç­ï¼Œç´°ç¯€ä¹Ÿè§£é‡‹çš„éå¸¸æ£’ï¼Œä½œè€…é‚„æœ‰è‡ªå·±å» trace Google ä»–å€‘é‡‹å‡ºçš„ word2vec C code
  * McCormick, C. (2016, April 19). Word2Vec Tutorial - The Skip-Gram Model. Retrieved from http://www.mccormickml.com
  * McCormick, C. (2017, January 11). Word2Vec Tutorial Part 2 - Negative Sampling. Retrieved from http://www.mccormickml.com
* [Tensorflow å®˜æ–¹æ–‡ä»¶](https://www.tensorflow.org/tutorials/word2vec)å¯ä»¥ç”¨é€™å€‹ç•¶å…¥é–€äº†è§£ word2vec çš„æ¦‚å¿µ
* word2vec paper
  * Mikolov, Tomas, et al. "Efficient estimation of word representations in vector space." arXiv preprint arXiv:1301.3781 (2013).
  * Mikolov, Tomas, et al. "Distributed representations of words and phrases and their compositionality." Advances in neural information processing systems. 2013.
* æ„Ÿè¬ [Aaron Chen](https://github.com/nailo2c) ç•¶æˆ‘ç¬¬ä¸€å€‹è®€è€…é‚„å¹«æˆ‘åµéŒ¯ ğŸ™

# Appendix 

$$ \sigma(x) = \frac{1}{1- e^{-x}} $$

$$ \frac{d \sigma(x)}{dx} = \sigma(x) (1 - \sigma(x))$$

$$ \sigma(x) + \sigma(-x) = 1 $$
